---
title: "DS_Project"
author: "Allison Shafer, Monica Puerto, Alison Ragan"
date: "10/27/2019"
output: html_document
---

```{r setup, include=FALSE}

# call in libraries to use

suppressMessages(library(tidyverse))
suppressMessages(library(DataExplorer))
suppressMessages(library(modelr))
suppressMessages(library(lubridate))
suppressMessages(library(stringr))
suppressMessages(library(purrr))
suppressMessages(library(gganimate))
suppressMessages(library(readxl))
suppressMessages(library(gifski))
suppressMessages(library(tidycensus))
suppressMessages(library(rvest))
suppressMessages(library(tmap))
suppressMessages(library(spData))
suppressMessages(library(tigris))
suppressMessages(library(sf))
suppressMessages(library(gridExtra))
suppressMessages(library(corrplot))
suppressMessages(library(dplyr))
suppressMessages(library(factoextra))
suppressMessages(library(cluster))

# 11/8: Added packages "tmap", "spData", "tigris"
# install.packages("tmap")
# install.packages("spData")
# install.packages("tigris")

# 11/14: Added package "factoextra"
# install.packages("factoextra")
# install.packages("cluster")
```
# Data Downloads
```{r download data - eviction dataset}

# set up a data directory if it does not exist already
# if(!file.exists("./data")) {dir.create("./data")}

# store urls needed - right click on excel symbol and select 
# copy link address
# eviction_US_all <- c("https://eviction-lab-data-downloads.s3.amazonaws.com/US/all.csv")

# check to see if URL saved to variable
# eviction_US_all

#download the csv file
# download.file(eviction_US_all, destfile = "./data/eviction_US_all.csv", mode = "wb") #this takes a bit ranges from 2-5 minutes, over bad WIFI might take 10 min

# read the csv file into R and save into dataframe
eviction_US_all <- read_csv("./data/eviction_US_all.csv")
names(eviction_US_all) <- gsub(x = names(eviction_US_all), pattern = "-", replacement = "_") 

dim(eviction_US_all)

head(eviction_US_all, 10)
```

```{r import rural urban}

# download and load urbal/rural classifications

# set up a data directory if it does not exist already
if(!file.exists("./data")) {dir.create("./data")}

# store urls needed - right click on excel symbol and select 
# copy link address
urban_rural <- c("http://www2.census.gov/geo/docs/reference/ua/County_Rural_Lookup.xlsx")

#download the .xlsx file
download.file(urban_rural, destfile = "./data/urban_rural.xlsx", mode = "wb")

#read xlsx into R
urban_rural <- read_excel(skip = 3, "./data/urban_rural.xlsx")

names(urban_rural)

# clean data set
urban_rural <- urban_rural %>%
  rename("Percent_Rural" = "2010 Census \r\nPercent Rural",
         "GEOID" = "2015 GEOID") 

# check names were updated
names(urban_rural)

# select only needed fields
urban_rural <-urban_rural %>%
  mutate(rural_flag = ifelse(round(Percent_Rural, 3) > 50, 1, 0)) %>%
  select(GEOID, State, Percent_Rural, rural_flag) 

# view cleaned up dataset
head(urban_rural, 10)
```

```{r Webscrape US Labor Stats}
if(!file.exists("./data/labor_data")) {dir.create("./data/labor_data")}

#save url into a variable
url <- "https://www.bls.gov/lau/#tables"

#download the html content using read_html
download.file(url,destfile="./data/uslabor.html")
us_county_labor_html <- read_html("./data/uslabor.html")

#extract the xslx 
us_county_labor_html %>% 
   rvest::html_nodes("ul") %>%
        rvest::html_nodes("li") %>%
        rvest::html_nodes("a") %>%
        rvest::html_attr("href") %>%
        str_subset(".xlsx$") -> us_labor_urls

#domain
domain <- "https://www.bls.gov"

#paste domain to urls
str_c(domain,us_labor_urls) -> us_labor_urls

#only need years from 2000 to 2016
us_labor_urls[3:19] -> us_labor_2000_2016
years <- rep(2000:2016,1)

#a for loop that downloads each file
for(i in seq_along(us_labor_2000_2016)){
  download.file(us_labor_2000_2016[i],destfile = paste("./data/labor_data/",years[i],".xslx",sep=""),mode="wb")
}

#save the files pertaining to us labor
labor_files <- dir("./data/labor_data")


#create a function that downloads each url and saves it #into a dataframe
read_files <- function(x){
read_excel(path= paste("./data/labor_data/",x,sep=""),skip = 7,col_names = c("laus_code","state_fips_code","county_fips_code","county_name","year","","labor_force","employed","unemployed","unemployment_rate"),sheet = 1,na="")
}

#map the function to read each file 
map(labor_files,read_files) -> all_labor_data

#join all the US labor tables
all_labor_data %>% reduce(full_join) -> all_labor_data
```
# Data Cleaning
``` {r clean up data set}
#remove some rows that have NA in Year and remove empty column next to year
filter(all_labor_data,!is.na(year)) %>% select(-6) -> all_labor_data
 
 
#creating a GEOID to join 
all_labor_data$GEOID <- str_c(all_labor_data$state_fips_code,all_labor_data$county_fips_code)
 
 
#change year to integer
all_labor_data$year <- as.integer(all_labor_data$year)
```

# Downloading Spatial Dataset

```{r download county spatial data}

# set up a data directory if it does not exist already
if(!file.exists("./data")) {dir.create("./data")}

#download county shapefiles
download.file("https://www2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_county_500k.zip", destfile = "./data/county_shp.zip")

spatial_data <- "./data/county_shp.zip"

unzip(spatial_data, overwrite = TRUE, exdir = "./data/spatial/county_shp")

county_boundaries <- st_read("./data/spatial/county_shp/cb_2018_us_county_500k.shp")
county_boundaries

```
# Creating Additional Datasets From Existing Data

```{r make county dataset}
#County Table joined with 2 additional variables : rural flag and unemployment rate

eviction_county <- eviction_US_all %>%
  right_join(urban_rural, key = "GEOID") %>% 
  left_join(select(all_labor_data,GEOID,year,unemployment_rate), by =c("GEOID" = "GEOID", "year"="year"))
         
head(eviction_county, 10)
```

```{r make state dataset}
eviction_state <- eviction_US_all %>%
  filter(nchar(GEOID) == 2)
         
head(eviction_state, 10)
```

```{r make region column}
states_regions <- as.data.frame(cbind(state.name,levels(state.region)))#creating state and region table
names(states_regions) <- c("states","region")
  
left_join(eviction_state, states_regions, by = c("name"="states")) -> eviction_state

left_join(eviction_county, states_regions,by = c("parent_location"="states")) -> eviction_county
```

```{r make pct_non_white column for county}
eviction_county$pct_nonwhite <- 100 - eviction_county$pct_white
```

# Data Exploration

```{r Chunk Nulls Exploration}
#We have 22.5% missing of eviction rate data in county and 11% missing in state. ALl 
missing_state <- plot_missing(eviction_state)
missing_eviction <- plot_missing(eviction_county)
#colSums(is.na(eviction_state))/nrow(eviction_state)
#colSums(is.na(eviction_county))/nrow(eviction_county)
grid.arrange(missing_state,missing_eviction,ncol=2)
```
```{r Check if any states missing Evictions}
eviction_state %>% group_by(GEOID,name) %>% summarise_at(c("eviction_filings", "evictions"), sum, na.rm = TRUE) %>% filter(evictions != 0)
```



```{r Correlation Matrix}
#plot_correlation(na.omit(eviction_county), maxcat = 5L)
na.omit(eviction_county) %>%
select_if(is.numeric) %>%
  as.matrix() %>% cor()  -> cor_matrix
corrplot(cor_matrix[,1:nrow(cor_matrix)][1:nrow(cor_matrix),21, drop=FALSE], cl.pos='n',cl.ratio = .2, method = "number", tl.srt = 45,tl.col = "black")
#corrplot(cor_matrix, type="upper", order="hclust", sig.level = 0.01, insig = "blank")
```



```{r data exploration, echo = false, eval = false}
#look at col names
colnames(eviction_US_all)
#look at summary of data
summary(eviction_US_all)
summary(all_labor_data)
```

```{r EDA county - eviction rate}
# get top 100 counties with highest average evistion rate
eviction_county %>%
  group_by(GEOID) %>%
  summarise(
    avg_rate = mean(eviction_filing_rate, na.rm = TRUE)
  ) %>%
  top_n(100) %>%
  arrange(desc(avg_rate))
# histogram of eviction rates in county dataset
eviction_county %>%
  group_by(eviction_filing_rate) %>%
  count() %>%
  ggplot(aes(eviction_filing_rate)) +
  geom_histogram()
```



```{r Distribution of Variables}
eviction_county %>%
  select(-year,-eviction_filings, -evictions,-imputed, -low_flag,-subbed) %>% 
  keep(is.numeric) %>%                     # Keep only numeric columns
  gather() %>%                             # Convert to key-value pairs
  ggplot(aes(value)) +                     # Plot the values
    facet_wrap(~ key, scales = "free") +   # In separate panels
    geom_density() 
```



```{r EDA state level data}
# look at dataset
names(eviction_state)
summary(eviction_state)
# look at 2016 
eviction_state %>%
  filter(year == 2016) %>%
  ggplot(aes(x = name, y = poverty_rate)) +
  geom_bar(stat = 'identity') +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  coord_flip()
```

```{r EDA county level data}
# look at data set
eviction_county %>%
  group_by(GEOID) 
ncol(eviction_county)
summary(eviction_county)
glimpse(eviction_county)
```

```{r gganimate eviction for state}
# look at eviction rate over time per state -- create gganimate
# this needs to have better formatting for final.
eviction_state %>%
  ggplot(aes(x = eviction_filing_rate, y = name)) +
  geom_point(aes(size = population, fill = rent_burden),
             shape = 21) +
  scale_x_log10(breaks = 2^(-1:7)*1000) +
  scale_size(range = c(1, 20), guide = FALSE) +
  labs(x = "Eviction Rate",
       y = "State") +
  transition_states(year, transition_length = 1,
                    state_length = 1)+
  ggtitle("Year showing {closest_state}",
          subtitle = "Frame {frame} of {nframes}")+
  theme_bw()
```

# Set up Spatial Datasets for Mapping
```{r set up spatial datasets, eval = FALSE}
# get Census TIGER shapefiles for state
state_us_geo <- tigris::states(class= "sf")
# add spatial data to the state level eviction data
eviction_by_state <- eviction_state %>%
                       left_join(state_us_geo, c("GEOID" = "GEOID"))
eviction_by_state
# add spatial data to the county level eviction data
eviction_by_county <- eviction_county %>%
                      left_join(county_boundaries, c("GEOID" = "GEOID"))
eviction_by_county
```

```{r top_bot_cty_fl_trends, eval=FALSE}
# Top + Bottom 10 Eviction Filing Rates -- determined by avg from 2000 to 2016
# calculate top/bottom ten dynamically 

# Calculate mean filing rate and remove any with missing data over time, or any with an avg rate less than 0.1
# Exclude states without data (02 (Alaska), 05 (Arkansas), 38 (North Dakota), 46 (South Dakota))
ecplot <- eviction_county %>%
  group_by(GEOID) %>% 
  mutate(avg_ev_fl_rate = mean(eviction_filing_rate, na.rm=TRUE)) %>% 
  filter(!any(is.na(eviction_filing_rate))) %>%
  filter(avg_ev_fl_rate >= 0.1) %>%
  filter(State != "AK" & State != "AR" & State != "ND" & State != "SD") 

# Sort in descending order and grab the GEOIDs with the highest filing rates
top_ec <- ecplot %>% 
  group_by(GEOID) %>% 
  arrange(desc(avg_ev_fl_rate))
top_ec <- c(unique(top_ec$GEOID))[1:10]

# Sort in ascending order and grab the GEOIDs with the lowest filing rates
bot_ec <- ecplot %>% 
  group_by(GEOID) %>% 
  arrange(avg_ev_fl_rate)
bot_ec <- c(unique(bot_ec$GEOID))[1:10]

# Create column for county + state
ecplot$County <- paste(tools::toTitleCase(ecplot$name), 
                       ecplot$State)
# Set Wes Anderson palette -- why is it not working??
# pal <- wes_palette("Zissou1", 100, type = "continuous")

# Plot the top filing counties over time
top_fl <- ecplot %>% 
  filter(GEOID %in% top_ec) %>% 
  ggplot() +
  geom_line(aes(x = year, 
                y = eviction_filing_rate, 
                group = County, 
                color = County)) +
  xlab("Year") +
  ylab("Eviction Filing Rates") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(plot.subtitle = element_text(hjust = 0.5)) +
  labs(title = "Top Filing Counties* Over Time",
       subtitle = "*Excludes counties with missing data",
       caption = "Data from evictionlab.org") #+
  # scale_fill_manual(values = wes_palette("Zissou1"))
  # scale_fill_gradientn(colours = pal)

# Plot the lowest filing counties over time 
bot_fl <- ecplot %>% 
  filter(GEOID %in% bot_ec) %>% 
  ggplot() +
  geom_line(aes(x = year, 
                y = eviction_filing_rate, 
                group = County, 
                color = County)) +
  xlab("Year") +
  ylab("Eviction Filing Rates") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(plot.subtitle = element_text(hjust = 0.5)) +
  labs(title = "Lowest Filing Counties* Over Time",
       subtitle = "*Excludes counties with missing data",
       caption = "Data from evictionlab.org") # +
  # scale_fill_manual(values = wes_palette("Zissou1"))
  # scale_fill_gradientn(colours = pal)
# Display plots
top_fl
bot_fl

# Also facet by region -- top 10 counties 
```

## K means clustering for determining similar counties

Why k-means?

- We're not trying to predict anything 
- We don't have an existing way to verify if counties are truly/correctly similar to one another

To perform a cluster analysis in R, generally, the data should be prepared as follows:

- Rows are observations (individuals) and columns are variables
- Any missing value in the data must be removed or estimated.
  - Thinking we exclude counties without any data 
- The data must be standardized (i.e., scaled) to make variables comparable

We need to make one row for each county. There are two ways to do this: 

* `group_by(GEOID)` and make each column the average of all the years 
  - this ignores how counties have changed over time
  - doesn't change the amount of variables 
* `spread_by(GEOID)` and give each column its own year (i.e. 2000 population, 2001 population, etc.)
  - results in many variables
  - kmeans can handle many, but may be extremely slow

We need to consider whether we want each year to have equal importance. 

Some weaknesses to keep in mind: 

- Requires us to pre-specify the number of clusters
- Sensitive to outliers

How to determine optimal clusters: 

1. elbow method
2. silhouette method
3. gap statistic
4. Use the `Nbclust` package which provides 30 indices for determining the relevant number of clusters and proposes to users the best clustering scheme from the different results obtained by varying all combinations of number of clusters, distance measures, and clustering methods.

```{r kmeans_avg}
km_avg <- eviction_county %>%
  group_by(GEOID) %>%
  mutate(average_population = mean(population, na.rm = TRUE)) %>%
  mutate(average_poverty_rate = mean(poverty_rate, na.rm = TRUE)) %>%
  mutate(average_pct_renter_occupied = mean(pct_renter_occupied, na.rm = TRUE)) %>%
  mutate(average_median_gross_rent = mean(median_gross_rent, na.rm = TRUE)) %>%
  mutate(average_median_household_income = mean(median_household_income, na.rm = TRUE)) %>%
  mutate(average_median_property_value = mean(median_property_value, na.rm = TRUE)) %>%
  mutate(average_rent_burden = mean(rent_burden, na.rm = TRUE)) %>%
  mutate(average_pct_white = mean(pct_white, na.rm = TRUE)) %>%
  mutate(average_pct_af_am = mean(pct_af_am, na.rm = TRUE)) %>%
  mutate(average_pct_hispanic = mean(pct_hispanic, na.rm = TRUE)) %>%
  mutate(average_pct_am_ind = mean(pct_am_ind, na.rm = TRUE)) %>%
  mutate(average_pct_asian = mean(pct_asian, na.rm = TRUE)) %>%
  mutate(average_pct_nh_pi = mean(pct_nh_pi, na.rm = TRUE)) %>%
  mutate(average_pct_multiple = mean(pct_multiple, na.rm = TRUE)) %>%
  mutate(average_pct_other = mean(pct_other, na.rm = TRUE)) %>%
  mutate(average_Percent_Rural = mean(Percent_Rural, na.rm = TRUE)) %>%
  mutate(average_unemployment_rate = mean(unemployment_rate, na.rm = TRUE)) %>%
  mutate(average_pct_nonwhite = mean(pct_nonwhite, na.rm = TRUE)) 

km_avg <- subset(km_avg, select = c("GEOID", "average_population", "average_poverty_rate", "average_pct_renter_occupied", "average_median_gross_rent", "average_median_household_income", "average_median_property_value", "average_rent_burden", "average_pct_white", "average_pct_af_am", "average_pct_hispanic", "average_pct_am_ind", "average_pct_asian", "average_pct_nh_pi", "average_pct_multiple", "average_pct_other", "average_Percent_Rural", "average_unemployment_rate", "average_pct_nonwhite" ))

# dedupe and drop nulls
km_avg <- km_avg %>%
  distinct() %>% # reduces to 3147
  drop_na() # reduces to 3138

# since kmeans is sensitive to outliers, drop outliers from model data 

# set GEOID (chr.) to index
km_avg <- km_avg %>%
  column_to_rownames(., var = "GEOID")

# scale numerical data
library(MASS)
ind <- sapply(km_avg, is.numeric)
km_avg[ind] <- lapply(km_avg[ind], scale)

# calculate and visualize distance 
# km_avg_distance <- get_dist(km_avg)
# fviz_dist(km_avg_distance, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))

# determine optimal clusters 
# elbow 
set.seed(123)
fviz_nbclust(km_avg, kmeans, method = "wss") # 3
# silhouette
fviz_nbclust(km_avg, kmeans, method = "silhouette") # 3

# Results
# Compute k-means clustering
set.seed(123)
km3 <- kmeans(km_avg, 3, nstart = 25)
print(km3)
fviz_cluster(km3, data = km_avg)

# Extract clusters and add to data frame
km_avg$cluster <- km3$cluster
# reintroduce GEOID as column
km_avg$GEOID <- c(row.names(km_avg))
# create separate df with just GEOID and cluster
geo_clust <- subset(km_avg, select=c("GEOID", "cluster"))

# plot cluster amounts
ggplot(data=km_avg, aes(x = cluster)) + geom_bar(stat="bin")
 
# Plot eviction filing rates, faceted by cluster
evr <- eviction_county %>%
  group_by(GEOID) %>%
  mutate(avg_fl = mean(eviction_filing_rate, na.rm=TRUE))
evr <- subset(evr, select = c("GEOID", "avg_fl"))
evr <- evr %>%
  distinct() %>%
  drop_na()
merge(km_avg, evr, by = "GEOID") %>%
  ggplot() + geom_bar(aes(x = GEOID, y = avg_fl), stat = 'identity') + facet_grid(. ~ cluster)
```

```{r kmeans_retain_year}
# make a list of columns to keep, drop those we won't
tot_cols <- c(names(eviction_county))
tot_cols <- tot_cols[-c(1, 2, 3, 4, 7, 21, 22, 23, 24, 25, 26, 27, 28, 30, 32)]
km_tot <- pivot_wider(eviction_county, names_from=year, values_from=c(population, median_gross_rent), names_sep="_") # not working
```
