---
title: "DS_Project"
author: "Allison Shafer, Monica Puerto, Alison Ragan"
date: "10/27/2019"
output: html_document
---

# UPDATE CODE

```{r setup, include=FALSE}

# call in libraries to use

suppressMessages(library(tidyverse))
suppressMessages(library(DataExplorer))
suppressMessages(library(modelr))
suppressMessages(library(lubridate))
suppressMessages(library(stringr))
suppressMessages(library(purrr))
suppressMessages(library(gganimate))
suppressMessages(library(readxl))
suppressMessages(library(gifski))
suppressMessages(library(tidycensus))
suppressMessages(library(rvest))
suppressMessages(library(tmap))
suppressMessages(library(spData))
suppressMessages(library(tigris))
suppressMessages(library(sf))
suppressMessages(library(gridExtra))
suppressMessages(library(corrplot))
suppressMessages(library(dplyr))

# 11/8: Added packages "tmap", "spData", "tigris"
# install.packages("tmap")
# install.packages("spData")
# install.packages("tigris")

```
# Data Downloads
```{r download data - eviction dataset}

# set up a data directory if it does not exist already
# if(!file.exists("./data")) {dir.create("./data")}

# store urls needed - right click on excel symbol and select 
# copy link address
# eviction_US_all <- c("https://eviction-lab-data-downloads.s3.amazonaws.com/US/all.csv")

# check to see if URL saved to variable
# eviction_US_all

#download the csv file
# download.file(eviction_US_all, destfile = "./data/eviction_US_all.csv", mode = "wb") #this takes a bit ranges from 2-5 minutes, over bad WIFI might take 10 min

# read the csv file into R and save into dataframe
eviction_US_all <- read_csv("./data/eviction_US_all.csv")
names(eviction_US_all) <- gsub(x = names(eviction_US_all), pattern = "-", replacement = "_") 

dim(eviction_US_all)

head(eviction_US_all, 10)
```

```{r import rural urban}

# download and load urbal/rural classifications

# set up a data directory if it does not exist already
if(!file.exists("./data")) {dir.create("./data")}

# store urls needed - right click on excel symbol and select 
# copy link address
urban_rural <- c("http://www2.census.gov/geo/docs/reference/ua/County_Rural_Lookup.xlsx")

#download the .xlsx file
download.file(urban_rural, destfile = "./data/urban_rural.xlsx", mode = "wb")

#read xlsx into R
urban_rural <- read_excel(skip = 3, "./data/urban_rural.xlsx")

names(urban_rural)

# clean data set
urban_rural <- urban_rural %>%
  rename("Percent_Rural" = "2010 Census \r\nPercent Rural",
         "GEOID" = "2015 GEOID") 

# check names were updated
names(urban_rural)

# select only needed fields
urban_rural <-urban_rural %>%
  mutate(rural_flag = ifelse(round(Percent_Rural, 3) > 50, 1, 0)) %>%
  select(GEOID, State, Percent_Rural, rural_flag) 

# view cleaned up dataset
head(urban_rural, 10)
```

```{r Webscrape US Labor Stats}
if(!file.exists("./data/labor_data")) {dir.create("./data/labor_data")}

#save url into a variable
url <- "https://www.bls.gov/lau/#tables"

#download the html content using read_html
download.file(url,destfile="./data/uslabor.html")
us_county_labor_html <- read_html("./data/uslabor.html")

#extract the xslx 
us_county_labor_html %>% 
   rvest::html_nodes("ul") %>%
        rvest::html_nodes("li") %>%
        rvest::html_nodes("a") %>%
        rvest::html_attr("href") %>%
        str_subset(".xlsx$") -> us_labor_urls

#domain
domain <- "https://www.bls.gov"

#paste domain to urls
str_c(domain,us_labor_urls) -> us_labor_urls

#only need years from 2000 to 2016
us_labor_urls[3:19] -> us_labor_2000_2016
years <- rep(2000:2016,1)

#a for loop that downloads each file
for(i in seq_along(us_labor_2000_2016)){
  download.file(us_labor_2000_2016[i],destfile = paste("./data/labor_data/",years[i],".xslx",sep=""),mode="wb")
}

#save the files pertaining to us labor
labor_files <- dir("./data/labor_data")


#create a function that downloads each url and saves it #into a dataframe
read_files <- function(x){
read_excel(path= paste("./data/labor_data/",x,sep=""),skip = 7,col_names = c("laus_code","state_fips_code","county_fips_code","county_name","year","","labor_force","employed","unemployed","unemployment_rate"),sheet = 1,na="")
}

#map the function to read each file 
map(labor_files,read_files) -> all_labor_data

#join all the US labor tables
all_labor_data %>% reduce(full_join) -> all_labor_data
```
# Data Cleaning
``` {r clean up data set}
#remove some rows that have NA in Year and remove empty column next to year
filter(all_labor_data,!is.na(year)) %>% select(-6) -> all_labor_data
 
 
#creating a GEOID to join 
all_labor_data$GEOID <- str_c(all_labor_data$state_fips_code,all_labor_data$county_fips_code)
 
 
#change year to integer
all_labor_data$year <- as.integer(all_labor_data$year)
```

# Downloading Spatial Dataset

```{r download county spatial data}

# set up a data directory if it does not exist already
if(!file.exists("./data")) {dir.create("./data")}

#download county shapefiles
download.file("https://www2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_county_500k.zip", destfile = "./data/county_shp.zip")

spatial_data <- "./data/county_shp.zip"

unzip(spatial_data, overwrite = TRUE, exdir = "./data/spatial/county_shp")

county_boundaries <- st_read("./data/spatial/county_shp/cb_2018_us_county_500k.shp")
county_boundaries

```
# Creating Additional Datasets From Existing Data

```{r make county dataset}
#County Table joined with 2 additional variables : rural flag and unemployment rate

eviction_county <- eviction_US_all %>%
  right_join(urban_rural, key = "GEOID") %>% 
  left_join(select(all_labor_data,GEOID,year,unemployment_rate), by =c("GEOID" = "GEOID", "year"="year"))
         
head(eviction_county, 10)
```

```{r make state dataset}
eviction_state <- eviction_US_all %>%
  filter(nchar(GEOID) == 2)
         
head(eviction_state, 10)
```

# Data Exploration

```{r data exploration, echo = false, eval = false}
#look at col names
colnames(eviction_US_all)

#look at unique values for each variable
# map(eviction_US_all, unique)
```

```{r Chunk Nulls Exploration}
#We have 22.5% missing of eviction rate data in county and 11% missing in state. ALl 
missing_state <- plot_missing(eviction_state)
missing_eviction <- plot_missing(eviction_county)
#colSums(is.na(eviction_state))/nrow(eviction_state)
#colSums(is.na(eviction_county))/nrow(eviction_county)
grid.arrange(missing_state,missing_eviction,ncol=2)
```

```{r Correlation Matrix}

#plot_correlation(na.omit(eviction_county), maxcat = 5L)
na.omit(eviction_county) %>%
select_if(is.numeric) %>%
  as.matrix() %>% cor()  -> cor_matrix


corrplot(cor_matrix[,1:nrow(cor_matrix)][1:nrow(cor_matrix),20, drop=FALSE], cl.pos='n',cl.ratio = .2, method = "number", tl.srt = 45,tl.col = "black")
#corrplot(cor_matrix, type="upper", order="hclust", sig.level = 0.01, insig = "blank")
```

```{r make region column}
states_regions <- as.data.frame(cbind(state.name,levels(state.region)))#creating state and region table
names(states_regions) <- c("states","region")
  
left_join(eviction_state, states_regions, by = c("name"="states")) -> eviction_state

left_join(eviction_county, states_regions,by = c("parent_location"="states")) -> eviction_county
```

```{r Race Other %}
#creating a new variable of nonwhite percentage
eviction_county$pct_nonwhite <- 100 - eviction_county$pct_white

na.omit(eviction_county) %>%
select_if(is.numeric) %>%
  as.matrix() %>% cor()  -> cor_matrix
```

```{r data exploration, echo = false, eval = false}
#look at col names
colnames(eviction_US_all)

#look at summary of data
summary(eviction_US_all)
summary(all_labor_data)
```

```{r EDA county - eviction rate}
# get top 100 counties with highest average evistion rate
eviction_county %>%
  group_by(GEOID) %>%
  summarise(
    avg_rate = mean(eviction_rate, na.rm = TRUE)
  ) %>%
  top_n(100) %>%
  arrange(desc(avg_rate))

# histogram of eviction rates in county dataset
eviction_county %>%
  group_by(eviction_rate) %>%
  count() %>%
  ggplot(aes(eviction_rate)) +
  geom_histogram()

```

```{r EDA state level data}
corrplot(cor_matrix[,1:nrow(cor_matrix)][1:nrow(cor_matrix),20, drop=FALSE], cl.pos='n',cl.ratio = .2, method = "number", tl.srt = 45,tl.col = "black")
```

```{r Distribution of Variables}
eviction_county %>%
  select(-year,-eviction_filing_rate,-eviction_filings, -evictions,-imputed, - low_flag,-subbed) %>% 
  keep(is.numeric) %>%                     # Keep only numeric columns
  gather() %>%                             # Convert to key-value pairs
  ggplot(aes(value)) +                     # Plot the values
    facet_wrap(~ key, scales = "free") +   # In separate panels
    geom_density() 
```

```{r plotting Y against X}
na.omit(eviction_county) %>%
  select_if(is.numeric) %>%
  gather(-eviction_rate,-population,-rent_burden, key = "var", value = "value") %>%
  ggplot(aes(x = value, y = eviction_rate, color = population)) +
    geom_point() +
    facet_wrap(~ var, scales = "free") +
    theme_bw()
```

```{r EDA state level data}

# look at dataset

names(eviction_state)

summary(eviction_state)

# look at 2016 
eviction_state %>%
  filter(year == 2016) %>%
  ggplot(aes(x = name, y = poverty_rate)) +
  geom_bar(stat = 'identity') +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  coord_flip()

```

```{r EDA county level data}

# look at data set
eviction_county %>%
  group_by(GEOID)

ncol(eviction_county)
summary(eviction_county)
glimpse(eviction_county)

```

```{r gganimate eviction for state}

# look at eviction rate over time per state -- create gganimate
# this needs to have better formatting for final.

eviction_state %>%
  ggplot(aes(x = eviction_rate, y = name)) +
  geom_point(aes(size = population, fill = rent_burden),
             shape = 21) +
  scale_x_log10(breaks = 2^(-1:7)*1000) +
  scale_size(range = c(1, 20), guide = FALSE) +
  labs(x = "Eviction Rate",
       y = "State") +
  transition_states(year, transition_length = 1,
                    state_length = 1)+
  ggtitle("Year showing {closest_state}",
          subtitle = "Frame {frame} of {nframes}")+
  theme_bw()
```

```{r AR top_and_bottom_county_trends, eval=FALSE}
# Top 10 Eviction Filing Rates -- determined by avg from 2000 to 2016
# Bottom 10 Eviction Filing Rates -- determined by avg from 2000 to 2016
# year vs filing rt
# colored by rural flag 
ev_ct <- eviction_county[!(eviction_county$eviction_filing_rate == NA)]
ev_ct <- subset(eviction_county, eviction_filing_rate != NA)

evct <- eviction_county %>%
  filter(!is.na(eviction_filing_rate))
# confirm dropped rows
plot_missing(evct)

top_tbl_avg_fl <- eviction_county %>%
  group_by(GEOID) %>%
  summarise(avg_fl_rate = mean(eviction_filing_rate, na.rm = TRUE)) %>%
  arrange(desc(avg_fl_rate)) %>%
  top_n(15)

# 1. Why are there 0? Should we drop them? 
# 2. Some of the values are from states without eviction rate data -- do we need to drop? 
# 3. Should we drop outliers? 
# 4. Those with gaps in data--should we drop those too? 
bot_tbl_avg_fl <- eviction_county %>%
  group_by(GEOID) %>%
  summarise(avg_fl_rate = mean(eviction_filing_rate, na.rm = TRUE)) %>%
  arrange(desc(avg_fl_rate)) %>%
  filter(avg_fl_rate > 0.000000000) %>%
  top_n(-10)

top_10_avg_fl <- c(top_tbl_avg_fl$GEOID)
bot_10_avg_fl <- c(bot_tbl_avg_fl$GEOID)

top <- c(top_tbl_avg_fl$GEOID)
bottom <- c(bot_tbl_avg_fl$GEOID)

tbl_avg_fl <- eviction_county %>%
  group_by(GEOID) %>%
  summarise(avg_fl_rate = mean(eviction_filing_rate, na.rm = TRUE)) %>%
  arrange(desc(avg_fl_rate)) 

ec <- merge(eviction_county, tbl_avg_fl, by="GEOID")

ec$County <- paste(ec$name, ec$State)

# Change legend title
# Filter for only counties without missing years 
top_fl <- ec %>% 
  filter(GEOID %in% top) %>% 
  ggplot() +
  geom_line(aes(x = year, y = eviction_filing_rate, group = County, color = County)) +
  xlab("Year") +
  ylab("Eviction Filing Rates") +
  ggtitle("Top Filing Counties Over Time") +
  theme(plot.title = element_text(hjust = 0.5)) 
bot_fl <- ec %>% 
  filter(GEOID %in% bottom) %>% 
  ggplot(aes(x = year, y = eviction_filing_rate, group = County, color = County)) +
  geom_line() +
  xlab("Year") +
  ylab("Eviction Filing Rates") +
  ggtitle("Bottom Filing Counties Over Time") +
  theme(plot.title = element_text(hjust = 0.5))

# make another graph without missing data calculated in top 10 (aka top 10 of those with complete data)
# look at populations of gapped counties

top_fl
bot_fl

# Also facet by region -- top 10 counties 
```

# Set up Spatial Datasets for Mapping
```{r set up spatial datasets}

# get Census TIGER shapefiles for state
state_us_geo <- tigris::states(class= "sf")

# add spatial data to the state level eviction data
eviction_by_state <- eviction_state %>%
                      left_join(state_us_geo, c("GEOID" = "GEOID"))

# add spatial data to the county level eviction data
eviction_by_county <- eviction_county %>%
                      left_join(county_boundaries, c("GEOID" = "GEOID"))
```

```{r AR stack_overflow_q}
df <- data.frame(rating = c(rnorm(10), rnorm(10, mean=.8)), 
                 year = factor(rep(c(2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009), each = 4)),
                 reap = factor(rep(c("A", "B", "C", "D"), each = 10)))
df[23:26,]$rating <- NA
ggplot(df) + geom_line(aes(x = year, y = rating, color = reap))
```