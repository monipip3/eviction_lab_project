---
title: "DS_Project"
author: "Allison Shafer, Monica Puerto, Alison Ragan"
date: "10/27/2019"
output:
  word_document: default
  html_document: default
---

# UPDATE CODE

```{r setup, include=FALSE}

# call in libraries to use

suppressMessages(library(tidyverse))
suppressMessages(library(DataExplorer))
suppressMessages(library(modelr))
suppressMessages(library(lubridate))
suppressMessages(library(stringr))
suppressMessages(library(purrr))
suppressMessages(library(gganimate))
suppressMessages(library(readxl))
suppressMessages(library(gifski))
suppressMessages(library(tidycensus))
suppressMessages(library(rvest))
# suppressMessages(library(tmap))
suppressMessages(library(spData))
suppressMessages(library(tigris))
suppressMessages(library(sf))
suppressMessages(library(gridExtra))
suppressMessages(library(corrplot))
suppressMessages(library(dplyr))
library(cluster)
library(factoextra)


# 11/8: Added packages "tmap", "spData", "tigris"
# install.packages("tmap")
# install.packages("spData")
# install.packages("tigris")

# 11/14: Added package "factoextra"
# install.packages("factoextra")
```
# Data Downloads
```{r download data - eviction dataset}

# # set up a data directory if it does not exist already
# if(!file.exists("./data")) {dir.create("./data")}
# 
# # store urls needed - right click on excel symbol and select 
# # copy link address
# eviction_US_all <- c("https://eviction-lab-data-downloads.s3.amazonaws.com/US/all.csv")
# 
# # check to see if URL saved to variable
# # eviction_US_all
# 
# #download the csv file
# download.file(eviction_US_all, destfile = "./data/eviction_US_all.csv", mode = "wb") #this takes a bit ranges from 2-5 minutes, over bad WIFI might take 10 min

# read the csv file into R and save into dataframe
eviction_US_all <- read_csv("./data/eviction_US_all.csv")
names(eviction_US_all) <- gsub(x = names(eviction_US_all), pattern = "-", replacement = "_") 

dim(eviction_US_all)

head(eviction_US_all, 10)
```

```{r import rural urban}

# download and load urbal/rural classifications

# set up a data directory if it does not exist already
if(!file.exists("./data")) {dir.create("./data")}

# store urls needed - right click on excel symbol and select 
# copy link address
urban_rural <- c("http://www2.census.gov/geo/docs/reference/ua/County_Rural_Lookup.xlsx")

#download the .xlsx file
download.file(urban_rural, destfile = "./data/urban_rural.xlsx", mode = "wb")

#read xlsx into R
urban_rural <- read_excel(skip = 3, "./data/urban_rural.xlsx")

names(urban_rural)

# clean data set
urban_rural <- urban_rural %>%
  rename("Percent_Rural" = "2010 Census \r\nPercent Rural",
         "GEOID" = "2015 GEOID") 

# check names were updated
names(urban_rural)

# select only needed fields
urban_rural <-urban_rural %>%
  mutate(rural_flag = ifelse(round(Percent_Rural, 3) > 50, 1, 0)) %>%
  select(GEOID, State, Percent_Rural, rural_flag) 

# view cleaned up dataset
head(urban_rural, 10)
```

```{r Webscrape US Labor Stats}
if(!file.exists("./data/labor_data")) {dir.create("./data/labor_data")}

#save url into a variable
url <- "https://www.bls.gov/lau/#tables"

#download the html content using read_html
download.file(url,destfile="./data/uslabor.html")
us_county_labor_html <- read_html("./data/uslabor.html")

#extract the xslx 
us_county_labor_html %>% 
   rvest::html_nodes("ul") %>%
        rvest::html_nodes("li") %>%
        rvest::html_nodes("a") %>%
        rvest::html_attr("href") %>%
        str_subset(".xlsx$") -> us_labor_urls

#domain
domain <- "https://www.bls.gov"

#paste domain to urls
str_c(domain,us_labor_urls) -> us_labor_urls

#only need years from 2000 to 2016
us_labor_urls[3:19] -> us_labor_2000_2016
years <- rep(2000:2016,1)

#a for loop that downloads each file
for(i in seq_along(us_labor_2000_2016)){
  download.file(us_labor_2000_2016[i],destfile = paste("./data/labor_data/",years[i],".xslx",sep=""),mode="wb")
}

#save the files pertaining to us labor
labor_files <- dir("./data/labor_data")


#create a function that downloads each url and saves it #into a dataframe
read_files <- function(x){
read_excel(path= paste("./data/labor_data/",x,sep=""),skip = 7,col_names = c("laus_code","state_fips_code","county_fips_code","county_name","year","","labor_force","employed","unemployed","unemployment_rate"),sheet = 1,na="")
}

#map the function to read each file 
map(labor_files,read_files) -> all_labor_data

#join all the US labor tables
all_labor_data %>% reduce(full_join) -> all_labor_data
```
# Data Cleaning
``` {r clean up data set}
#remove some rows that have NA in Year and remove empty column next to year
filter(all_labor_data,!is.na(year)) %>% select(-6) -> all_labor_data
 
 
#creating a GEOID to join 
all_labor_data$GEOID <- str_c(all_labor_data$state_fips_code,all_labor_data$county_fips_code)
 
 
#change year to integer
all_labor_data$year <- as.integer(all_labor_data$year)
```

# Downloading Spatial Dataset

```{r download county spatial data}

# set up a data directory if it does not exist already
if(!file.exists("./data")) {dir.create("./data")}

#download county shapefiles
download.file("https://www2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_county_500k.zip", destfile = "./data/county_shp.zip")

spatial_data <- "./data/county_shp.zip"

unzip(spatial_data, overwrite = TRUE, exdir = "./data/spatial/county_shp")

county_boundaries <- st_read("./data/spatial/county_shp/cb_2018_us_county_500k.shp")
county_boundaries

```
# Creating Additional Datasets From Existing Data

```{r make county dataset}
#County Table joined with 2 additional variables : rural flag and unemployment rate

eviction_county <- eviction_US_all %>%
  right_join(urban_rural, key = "GEOID") %>% 
  left_join(select(all_labor_data,GEOID,year,unemployment_rate), by =c("GEOID" = "GEOID", "year"="year"))
         
head(eviction_county, 10)
```

```{r make state dataset}
eviction_state <- eviction_US_all %>%
  filter(nchar(GEOID) == 2)
         
head(eviction_state, 10)
```

# Data Exploration

```{r Chunk Nulls Exploration}
#We have 22.5% missing of eviction rate data in county and 11% missing in state. ALl 
missing_state <- plot_missing(eviction_state)
missing_eviction <- plot_missing(eviction_county)
#colSums(is.na(eviction_state))/nrow(eviction_state)
#colSums(is.na(eviction_county))/nrow(eviction_county)
grid.arrange(missing_state,missing_eviction,ncol=2)
```

```{r Correlation Matrix}

#plot_correlation(na.omit(eviction_county), maxcat = 5L)
na.omit(eviction_county) %>%
select_if(is.numeric) %>%
  as.matrix() %>% cor()  -> cor_matrix


corrplot(cor_matrix[,1:nrow(cor_matrix)][1:nrow(cor_matrix),20, drop=FALSE], cl.pos='n',cl.ratio = .2, method = "number", tl.srt = 45,tl.col = "black")
#corrplot(cor_matrix, type="upper", order="hclust", sig.level = 0.01, insig = "blank")
```

```{r make region column}
states_regions <- as.data.frame(cbind(state.name,levels(state.region)))#creating state and region table
names(states_regions) <- c("states","region")
  
left_join(eviction_state, states_regions, by = c("name"="states")) -> eviction_state

left_join(eviction_county, states_regions,by = c("parent_location"="states")) -> eviction_county
```

```{r Race Other %}
#creating a new variable of nonwhite percentage
eviction_county$pct_nonwhite <- 100 - eviction_county$pct_white

na.omit(eviction_county) %>%
select_if(is.numeric) %>%
  as.matrix() %>% cor()  -> cor_matrix
```

```{r EDA county - eviction rate}
# get top 100 counties with highest average evistion rate
eviction_county %>%
  group_by(GEOID) %>%
  summarise(
    avg_rate = mean(eviction_rate, na.rm = TRUE)
  ) %>%
  top_n(100) %>%
  arrange(desc(avg_rate))

# histogram of eviction rates in county dataset
eviction_county %>%
  group_by(eviction_rate) %>%
  count() %>%
  ggplot(aes(eviction_rate)) +
  geom_histogram()

```

```{r EDA corr}
corrplot(cor_matrix[,1:nrow(cor_matrix)][1:nrow(cor_matrix),20, drop=FALSE], cl.pos='n',cl.ratio = .2, method = "number", tl.srt = 45,tl.col = "black")
```

```{r Distribution of Variables}
eviction_county %>%
  select(-year,-eviction_filing_rate,-eviction_filings, -evictions,-imputed, - low_flag,-subbed) %>% 
  keep(is.numeric) %>%                     # Keep only numeric columns
  gather() %>%                             # Convert to key-value pairs
  ggplot(aes(value)) +                     # Plot the values
    facet_wrap(~ key, scales = "free") +   # In separate panels
    geom_density() 
```

```{r plotting Y against X}
na.omit(eviction_county) %>%
  select_if(is.numeric) %>%
  gather(-eviction_rate,-population,-rent_burden, key = "var", value = "value") %>%
  ggplot(aes(x = value, y = eviction_rate, color = population)) +
    geom_point() +
    facet_wrap(~ var, scales = "free") +
    theme_bw()
```

```{r EDA county level data}

# look at data set
eviction_county %>%
  group_by(GEOID)

ncol(eviction_county)
summary(eviction_county)
glimpse(eviction_county)

```

```{r AR top_bot_cty_fl_trends}
# Top + Bottom 10 Eviction Filing Rates -- determined by avg from 2000 to 2016
# ADD FUNCTIONALITY TO FACET BY REGION, TRY TO DO ALL THIS IN ONE PIPE
# calculate top/bottom ten dynamically 

# Calculate mean filing rate and remove any with missing data over time, or any with an avg rate less than 0.1
# Exclude states without data (02 (Alaska), 05 (Arkansas), 38 (North Dakota), 46 (South Dakota))
ecplot <- eviction_county %>%
  group_by(GEOID) %>% 
  mutate(avg_ev_fl_rate = mean(eviction_filing_rate, na.rm=TRUE)) %>% 
  filter(!any(is.na(eviction_filing_rate))) %>%
  filter(avg_ev_fl_rate >= 0.1) %>%
  filter(State != "AK" & State != "AR" & State != "ND" & State != "SD") 

# Sort in descending order and grab the GEOIDs with the highest filing rates
top_ec <- ecplot %>% 
  group_by(GEOID) %>% 
  arrange(desc(avg_ev_fl_rate))
top_ec <- c(unique(top_ec$GEOID))[1:10]

# Sort in ascending order and grab the GEOIDs with the lowest filing rates
bot_ec <- ecplot %>% 
  group_by(GEOID) %>% 
  arrange(avg_ev_fl_rate)
bot_ec <- c(unique(bot_ec$GEOID))[1:10]

# Create column for county + state
ecplot$County <- paste(tools::toTitleCase(ecplot$name), 
                       ecplot$State)
# Set Wes Anderson palette -- why is it not working??
# pal <- wes_palette("Zissou1", 100, type = "continuous")

# Plot the top filing counties over time
top_fl <- ecplot %>% 
  filter(GEOID %in% top_ec) %>% 
  ggplot() +
  geom_line(aes(x = year, 
                y = eviction_filing_rate, 
                group = County, 
                color = County)) +
  xlab("Year") +
  ylab("Eviction Filing Rates") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(plot.subtitle = element_text(hjust = 0.5)) +
  labs(title = "Top Filing Counties* Over Time",
       subtitle = "*Excludes counties with missing data",
       caption = "Data from evictionlab.org") #+
  # scale_fill_manual(values = wes_palette("Zissou1"))
  # scale_fill_gradientn(colours = pal)

# Plot the lowest filing counties over time 
bot_fl <- ecplot %>% 
  filter(GEOID %in% bot_ec) %>% 
  ggplot() +
  geom_line(aes(x = year, 
                y = eviction_filing_rate, 
                group = County, 
                color = County)) +
  xlab("Year") +
  ylab("Eviction Filing Rates") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(plot.subtitle = element_text(hjust = 0.5)) +
  labs(title = "Lowest Filing Counties* Over Time",
       subtitle = "*Excludes counties with missing data",
       caption = "Data from evictionlab.org") # +
  # scale_fill_manual(values = wes_palette("Zissou1"))
  # scale_fill_gradientn(colours = pal)
# Display plots
top_fl
bot_fl

# Also facet by region -- top 10 counties 
```

## K means clustering for determining similar counties

Why k-means?

- We're not trying to predict anything 
- We don't have an existing way to verify if counties are truly/correctly similar to one another

To perform a cluster analysis in R, generally, the data should be prepared as follows:

- Rows are observations (individuals) and columns are variables
- Any missing value in the data must be removed or estimated.
  - Thinking we exclude counties without any data 
- The data must be standardized (i.e., scaled) to make variables comparable

We need to make one row for each county. There are two ways to do this: 

* `group_by(GEOID)` and make each column the average of all the years 
  - this ignores how counties have changed over time
  - doesn't change the amount of variables 
* `gather_by(GEOID)` and give each column its own year (i.e. 2000 population, 2001 population, etc.)
  - results in many variables
  - kmeans can handle many, but may be slow

We need to consider whether we want each year to have equal importance. 

For clustering distance measures, there are a few different types. The most common are Euclidean and Manhattan, and other measures can be correlation-based distances.

Some weaknesses to keep in mind: 

- Requires us to pre-specify the number of clusters
- Sensitive to outliers

How to determine optimal clusters: 

1. elbow method
  goal: minimize total within cluster sum of square
  use the following algorithm to define the optimal clusters:
    Compute clustering algorithm (e.g., k-means clustering) for different values of k. For instance, by varying k from 1 to 10 clusters
    For each k, calculate the total within-cluster sum of square (wss)
    Plot the curve of wss according to the number of clusters k.
    The location of a bend (knee) in the plot is generally considered as an indicator of the appropriate number of clusters.
2. silhouette method
  In short, the average silhouette approach measures the quality of a clustering. That is, it determines how well each object lies within its cluster. A high average silhouette width indicates a good clustering. The average silhouette method computes the average silhouette of observations for different values of k. The optimal number of clusters k is the one that maximizes the average silhouette over a range of possible values for k.2
3. gap statistic
  The gap statistic compares the total intracluster variation for different values of k with their expected values under null reference distribution of the data (i.e. a distribution with no obvious clustering). The reference dataset is generated using Monte Carlo simulations of the sampling process. 
  In short, the algorithm involves the following steps:
    1. Cluster the observed data, varying the number of clusters from k=1,…,kmax, and compute the corresponding Wk.
    2. Generate B reference data sets and cluster each of them with varying number of clusters k=1,…,kmax. Compute the estimated gap statistics presented in eq. 9. 
    3. Let ¯w=(1/B)∑blog(W∗kb), compute the standard deviation sd(k)=√(1/b)∑b(log(W∗kb)−¯w)2 and define sk=sdk×√1+1/B
    4. Choose the number of clusters as the smallest k such that Gap(k)≥Gap(k+1)−sk+1
4. Use the `Nbclust` package which provides 30 indices for determining the relevant number of clusters and proposes to users the best clustering scheme from the different results obtained by varying all combinations of number of clusters, distance measures, and clustering methods.

```{r kmeans_test}
# one row == one county (and each col. is county average)
# limit to just one state for initial test
km_ec <- eviction_county %>%
  filter(State == "WA") %>%
  group_by(GEOID) %>%
  mutate(avg_pov_rate = mean(poverty_rate, na.rm = TRUE)) %>%
  mutate(avg_pop = mean(population, na.rm = TRUE)) %>%
  mutate(avg_pct_rent_occ = mean(pct_renter_occupied, na.rm = TRUE))

# quick test -- only use three params to cluster GEOID
km_ec_test <- subset(km_ec, select = c("GEOID", "avg_pov_rate", "avg_pop", "avg_pct_rent_occ"))

# dedupe and drop nulls
km_ec_test <- km_ec_test %>%
  distinct() %>%
  drop_na()

# set GEOID (chr.) to index
km_ec_test <- km_ec_test %>%
  column_to_rownames(., var = "GEOID")

# scale numerical data
library(MASS)
ind <- sapply(km_ec_test, is.numeric)
km_ec_test[ind] <- lapply(km_ec_test[ind], scale)

# calculate and visualize distance 
km_ec_distance <- get_dist(km_ec_test)
fviz_dist(km_ec_distance, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))

# determine optimal clusters 
# elbow 
set.seed(123)
fviz_nbclust(km_ec_test, kmeans, method = "wss")
# silhouette
fviz_nbclust(km_ec_test, kmeans, method = "silhouette")
# gap statistic 
set.seed(123)
gap_stat <- clusGap(km_ec_test, FUN = kmeans, nstart = 25, K.max = 10, B = 50)
# Print the result
print(gap_stat, method = "firstmax")
fviz_gap_stat(gap_stat)

# Results
# Compute k-means clustering with k = 4
set.seed(123)
final <- kmeans(km_ec_test, 4, nstart = 25)
print(final)
fviz_cluster(final, data = km_ec_test)

# Extract clusters and add to data frame -- HOW TO NOT REMOVE GEOID?? TF R? 
km_test_final <- km_ec_test %>%
  mutate(Cluster = final$cluster)

# Plot eviction filing rates, faceted by cluster
# Join evic. rates with data frame and re-add GEOID as column (don't forget to remove nulls)
```

```{r kmeans_avg}
km_avg <- eviction_county
```

```{r kmeans_retain_year}
km_gather <- eviction_county
```