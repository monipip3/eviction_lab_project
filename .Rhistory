us_county_labor_html <- read_html("./data/uslabor.html")
#extract the xslx
us_county_labor_html %>%
rvest::html_nodes("ul") %>%
rvest::html_nodes("li") %>%
rvest::html_nodes("a") %>%
rvest::html_attr("href") %>%
str_subset(".xlsx$") -> us_labor_urls
#domain
domain <- "https://www.bls.gov"
#paste domain to urls
str_c(domain,us_labor_urls) -> us_labor_urls
#only need years from 2000 to 2016
us_labor_urls[3:19] -> us_labor_2000_2016
years <- rep(2000:2016,1)
#a for loop that downloads each file
for(i in seq_along(us_labor_2000_2016)){
download.file(us_labor_2000_2016[i],destfile = paste("./data/labor_data/",years[i],".xslx",sep=""),mode="wb")
}
#save the files pertaining to us labor
labor_files <- dir("./data/labor_data")
#create a function that downloads each url and saves it #into a dataframe
read_files <- function(x){
read_excel(path= paste("./data/labor_data/",x,sep=""),skip = 7,col_names = c("laus_code","state_fips_code","county_fips_code","county_name","year","","labor_force","employed","unemployed","unemployment_rate"),sheet = 1,na="")
}
#map the function to read each file
map(labor_files,read_files) -> all_labor_data
read_files <- function(x){
read_excel(path= paste("./data/labor_data/",x,sep=""),skip = 7,col_names = c("laus_code","state_fips_code","county_fips_code","county_name","year","","labor_force","employed","unemployed","unemployment_rate"),sheet = 1,na="")
}
#map the function to read each file
map(labor_files,read_files) -> all_labor_data
all_labor_data %>% reduce(full_join) -> all_labor_data
if(!file.exists("./data/labor_data")) {dir.create("./data/labor_data")}
#save url into a variable
url <- "https://www.bls.gov/lau/#tables"
#download the html content using read_html
download.file(url,destfile="./data/uslabor.html")
us_county_labor_html <- read_html("./data/uslabor.html")
#extract the xslx
us_county_labor_html %>%
rvest::html_nodes("ul") %>%
rvest::html_nodes("li") %>%
rvest::html_nodes("a") %>%
rvest::html_attr("href") %>%
str_subset(".xlsx$") -> us_labor_urls
#domain
domain <- "https://www.bls.gov"
#paste domain to urls
str_c(domain,us_labor_urls) -> us_labor_urls
#only need years from 2000 to 2016
us_labor_urls[3:19] -> us_labor_2000_2016
years <- rep(2000:2016,1)
#a for loop that downloads each file
for(i in seq_along(us_labor_2000_2016)){
download.file(us_labor_2000_2016[i],destfile = paste("./data/labor_data/",years[i],".xslx",sep=""),mode="wb")
}
#save the files pertaining to us labor
labor_files <- dir("./data/labor_data")
#create a function that downloads each url and saves it #into a dataframe
read_files <- function(x){
read_excel(path= paste("./data/labor_data/",x,sep=""),skip = 7,col_names = c("laus_code","state_fips_code","county_fips_code","county_name","year","","labor_force","employed","unemployed","unemployment_rate"),sheet = 1,na="")
}
#map the function to read each file
map(labor_files,read_files) -> all_labor_data
#remove some rows that have NA in Year and remove empty column next to year
filter(all_labor_data,!is.na(year)) %>% select(-6) -> all_labor_data
map(labor_files,read_files) -> all_labor_data
labor_files <- dir("./data/labor_data")
read_files <- function(x){
read_excel(path= paste("./data/labor_data/",x,sep=""),skip = 7,col_names = c("laus_code","state_fips_code","county_fips_code","county_name","year","","labor_force","employed","unemployed","unemployment_rate"),sheet = 1,na="")
}
map(labor_files,read_files) -> all_labor_data
read_files <- function(x){
read_excel(path= paste("./data/labor_data/",x,sep=""),skip = 7,col_names = c("laus_code","state_fips_code","county_fips_code","county_name","year","","labor_force","employed","unemployed","unemployment_rate"),sheet = 1,na="")
}
labor_files <- dir("./data/labor_data")
map(labor_files,read_files) -> all_labor_data
if(!file.exists("./data/labor_data")) {dir.create("./data/labor_data")}
#save url into a variable
url <- "https://www.bls.gov/lau/#tables"
#download the html content using read_html
download.file(url,destfile="./data/uslabor.html")
us_county_labor_html <- read_html("./data/uslabor.html")
#extract the xslx
us_county_labor_html %>%
rvest::html_nodes("ul") %>%
rvest::html_nodes("li") %>%
rvest::html_nodes("a") %>%
rvest::html_attr("href") %>%
str_subset(".xlsx$") -> us_labor_urls
#domain
domain <- "https://www.bls.gov"
#paste domain to urls
str_c(domain,us_labor_urls) -> us_labor_urls
#only need years from 2000 to 2016
us_labor_urls[3:19] -> us_labor_2000_2016
years <- rep(2000:2016,1)
#a for loop that downloads each file
for(i in seq_along(us_labor_2000_2016)){
download.file(us_labor_2000_2016[i],destfile = paste("./data/labor_data/",years[i],".xslx",sep=""),mode="wb")
}
#save the files pertaining to us labor
labor_files <- dir("./data/labor_data")
#create a function that downloads each url and saves it #into a dataframe
read_files <- function(x){
read_excel(path= paste("./data/labor_data/",x,sep=""),skip = 7,col_names = c("laus_code","state_fips_code","county_fips_code","county_name","year","","labor_force","employed","unemployed","unemployment_rate"),sheet = 1,na="")
}
#map the function to read each file
map(labor_files,read_files) -> all_labor_data
labor_files
getwd()
getwd()
getwd()
getwd()
getwd()
```{r Webscrape US Labor Stats}
getwd()
if(!file.exists("./data/labor_data")) {dir.create("./data/labor_data")}
#save url into a variable
url <- "https://www.bls.gov/lau/#tables"
#download the html content using read_html
download.file(url,destfile="./data/uslabor.html")
us_county_labor_html <- read_html("./data/uslabor.html")
#extract the xslx
us_county_labor_html %>%
rvest::html_nodes("ul") %>%
rvest::html_nodes("li") %>%
rvest::html_nodes("a") %>%
rvest::html_attr("href") %>%
str_subset(".xlsx$") -> us_labor_urls
#domain
domain <- "https://www.bls.gov"
#paste domain to urls
str_c(domain,us_labor_urls) -> us_labor_urls
#only need years from 2000 to 2016
us_labor_urls[3:19] -> us_labor_2000_2016
years <- rep(2000:2016,1)
#a for loop that downloads each file
for(i in seq_along(us_labor_2000_2016)){
download.file(us_labor_2000_2016[i],destfile = paste("./data/labor_data/",years[i],".xslx",sep=""),mode="wb")
}
#save the files pertaining to us labor
labor_files <- dir("./data/labor_data")
read_files <- function(x){
read_excel(path= paste("./data/labor_data/",x,sep=""),skip = 7,col_names = c("laus_code","state_fips_code","county_fips_code","county_name","year","","labor_force","employed","unemployed","unemployment_rate"),sheet = 1,na="")
}
#map the function to read each file
map(labor_files,read_files) -> all_labor_data
getwd()
getwd()
#map the function to read each file
map(labor_files,read_files) -> all_labor_data
all_labor_data <- map(labor_files, read_files)
getwd()
if(!file.exists("./data/labor_data")) {dir.create("./data/labor_data")}
#save url into a variable
url <- "https://www.bls.gov/lau/#tables"
#download the html content using read_html
download.file(url,destfile="./data/uslabor.html")
us_county_labor_html <- read_html("./data/uslabor.html")
#extract the xslx
us_county_labor_html %>%
rvest::html_nodes("ul") %>%
rvest::html_nodes("li") %>%
rvest::html_nodes("a") %>%
rvest::html_attr("href") %>%
str_subset(".xlsx$") -> us_labor_urls
#domain
domain <- "https://www.bls.gov"
#paste domain to urls
str_c(domain,us_labor_urls) -> us_labor_urls
#only need years from 2000 to 2016
us_labor_urls[3:19] -> us_labor_2000_2016
years <- rep(2000:2016,1)
#a for loop that downloads each file
for(i in seq_along(us_labor_2000_2016)){
download.file(us_labor_2000_2016[i],destfile = paste("./data/labor_data/",years[i],".xslx",sep=""),mode="wb")
}
#save the files pertaining to us labor
labor_files <- dir("./data/labor_data")
getwd()
#create a function that downloads each url and saves it #into a dataframe
read_files <- function(x){
read_excel(path= paste("./data/labor_data/",x,sep=""),skip = 7,col_names = c("laus_code","state_fips_code","county_fips_code","county_name","year","","labor_force","employed","unemployed","unemployment_rate"),sheet = 1,na="")
}
#map the function to read each file
all_labor_data <- map(labor_files, read_files)
#only need years from 2000 to 2016
us_labor_urls[3:19] -> us_labor_2000_2016
years <- rep(2000:2016,1)
years <- as.character(years)
#a for loop that downloads each file
for(i in seq_along(us_labor_2000_2016)){
download.file(us_labor_2000_2016[i],destfile = paste("./data/labor_data/y",years[i],".xslx",sep=""),mode="wb")
}
#save the files pertaining to us labor
labor_files <- dir("./data/labor_data")
getwd()
#create a function that downloads each url and saves it #into a dataframe
read_files <- function(x){
read_excel(path= paste("./data/labor_data/",x,sep=""),skip = 7,col_names = c("laus_code","state_fips_code","county_fips_code","county_name","year","","labor_force","employed","unemployed","unemployment_rate"),sheet = 1, na="")
}
#map the function to read each file
all_labor_data <- map(labor_files, read_files)
read_files <- function(x){
read_excel(path= paste("./data/labor_data/y",x,sep=""),skip = 7,col_names = c("laus_code","state_fips_code","county_fips_code","county_name","year","","labor_force","employed","unemployed","unemployment_rate"),sheet = 1, na="")
}
#map the function to read each file
all_labor_data <- map(labor_files, read_files)
labor_files <- dir("./data/labor_data")
read_files <- function(x){
read_excel(path= paste("./data/labor_data/y",x,sep=""),skip = 7,col_names = c("laus_code","state_fips_code","county_fips_code","county_name","year","","labor_force","employed","unemployed","unemployment_rate"),sheet = 1, na="")
}
#map the function to read each file
all_labor_data <- map(labor_files, read_files)
suppressMessages(library(tidyverse))
suppressMessages(library(DataExplorer))
suppressMessages(library(modelr))
suppressMessages(library(lubridate))
suppressMessages(library(stringr))
suppressMessages(library(gganimate))
suppressMessages(library(readxl))
suppressMessages(library(gifski))
suppressMessages(library(tidycensus))
suppressMessages(library(rvest))
suppressMessages(library(tmap))
suppressMessages(library(spData))
suppressMessages(library(tigris))
suppressMessages(library(sf))
suppressMessages(library(gridExtra))
suppressMessages(library(corrplot))
suppressMessages(library(maps))
suppressMessages(library(purrr))
map(labor_files, read_files) -> all_labor_data
uninstall.packages("maps")
remove.packages(maps)
suppressMessages(library(tidyverse))
suppressMessages(library(DataExplorer))
suppressMessages(library(modelr))
suppressMessages(library(lubridate))
suppressMessages(library(stringr))
suppressMessages(library(gganimate))
suppressMessages(library(readxl))
suppressMessages(library(gifski))
suppressMessages(library(tidycensus))
suppressMessages(library(rvest))
suppressMessages(library(tmap))
suppressMessages(library(spData))
suppressMessages(library(tigris))
suppressMessages(library(sf))
suppressMessages(library(gridExtra))
suppressMessages(library(corrplot))
suppressMessages(library(purrr))
getwd()
if(!file.exists("./data/labor_data")) {dir.create("./data/labor_data")}
#save url into a variable
url <- "https://www.bls.gov/lau/#tables"
#download the html content using read_html
download.file(url,destfile="./data/uslabor.html")
us_county_labor_html <- read_html("./data/uslabor.html")
#extract the xslx
us_county_labor_html %>%
rvest::html_nodes("ul") %>%
rvest::html_nodes("li") %>%
rvest::html_nodes("a") %>%
rvest::html_attr("href") %>%
str_subset(".xlsx$") -> us_labor_urls
#domain
domain <- "https://www.bls.gov"
#paste domain to urls
str_c(domain,us_labor_urls) -> us_labor_urls
#only need years from 2000 to 2016
us_labor_urls[3:19] -> us_labor_2000_2016
years <- rep(2000:2016,1)
years <- as.character(years)
#a for loop that downloads each file
for(i in seq_along(us_labor_2000_2016)){
download.file(us_labor_2000_2016[i],destfile = paste("./data/labor_data/y",years[i],".xslx",sep=""),mode="wb")
}
#save the files pertaining to us labor
labor_files <- dir("./data/labor_data")
getwd()
#create a function that downloads each url and saves it #into a dataframe
read_files <- function(x){
read_excel(path= paste("./data/labor_data/y",x,sep=""),skip = 7,col_names = c("laus_code","state_fips_code","county_fips_code","county_name","year","","labor_force","employed","unemployed","unemployment_rate"),sheet = 1, na="")
}
#map the function to read each file
map(labor_files, read_files) -> all_labor_data
if(!file.exists("./data/labor_data")) {dir.create("./data/labor_data")}
#save url into a variable
url <- "https://www.bls.gov/lau/#tables"
#download the html content using read_html
download.file(url,destfile="./data/uslabor.html")
us_county_labor_html <- read_html("./data/uslabor.html")
#extract the xslx
us_county_labor_html %>%
rvest::html_nodes("ul") %>%
rvest::html_nodes("li") %>%
rvest::html_nodes("a") %>%
rvest::html_attr("href") %>%
str_subset(".xlsx$") -> us_labor_urls
#domain
domain <- "https://www.bls.gov"
#paste domain to urls
str_c(domain,us_labor_urls) -> us_labor_urls
#only need years from 2000 to 2016
us_labor_urls[3:19] -> us_labor_2000_2016
years <- rep(2000:2016,1)
years <- as.character(years)
#a for loop that downloads each file
for(i in seq_along(us_labor_2000_2016)){
download.file(us_labor_2000_2016[i],destfile = paste("./data/labor_data/y",years[i],".xslx",sep=""),mode="wb")
}
#save the files pertaining to us labor
labor_files <- dir("./data/labor_data")
getwd()
#create a function that downloads each url and saves it #into a dataframe
read_files <- function(x){
read_excel(path= paste("./data/labor_data/y",x,sep=""),skip = 7,col_names = c("laus_code","state_fips_code","county_fips_code","county_name","year","","labor_force","employed","unemployed","unemployment_rate"),sheet = 1, na="")
}
#map the function to read each file
map(labor_files, read_files) -> all_labor_data
getwd()
# call in libraries to use
suppressMessages(library(tidyverse))
suppressMessages(library(DataExplorer))
suppressMessages(library(modelr))
suppressMessages(library(lubridate))
suppressMessages(library(stringr))
suppressMessages(library(gganimate))
suppressMessages(library(readxl))
suppressMessages(library(gifski))
suppressMessages(library(tidycensus))
suppressMessages(library(rvest))
suppressMessages(library(tmap))
suppressMessages(library(spData))
suppressMessages(library(tigris))
suppressMessages(library(sf))
suppressMessages(library(gridExtra))
suppressMessages(library(corrplot))
suppressMessages(library(purrr))
# 11/8: Added packages "tmap", "spData", "tigris"
#install.packages("tmap")
#install.packages("spData")
#install.packages("tigris")
#install.packages("DataExplorer")
#install.packages("gridExtra")
#install.packages("corrplot")
#11/11: Added packages
#install.packages("maps")
# set up a data directory if it does not exist already
if(!file.exists("./data")) {dir.create("./data")}
# store urls needed - right click on excel symbol and select
# copy link address
eviction_US_all <- c("https://eviction-lab-data-downloads.s3.amazonaws.com/US/all.csv")
# check to see if URL saved to variable
eviction_US_all
#download the csv file
download.file(eviction_US_all, destfile = "./data/eviction_US_all.csv", mode = "wb") #this takes a bit ranges from 2-5 minutes, over bad WIFI might take 10 min
# read the csv file into R and save into dataframe
eviction_US_all <- read_csv("./data/eviction_US_all.csv")
# set up a data directory if it does not exist already
if(!file.exists("./data")) {dir.create("./data")}
# store urls needed - right click on excel symbol and select
# copy link address
eviction_US_all <- c("https://eviction-lab-data-downloads.s3.amazonaws.com/US/all.csv")
# check to see if URL saved to variable
eviction_US_all
#download the csv file
download.file(eviction_US_all, destfile = "./data/eviction_US_all.csv", mode = "wb") #this takes a bit ranges from 2-5 minutes, over bad WIFI might take 10 min
# call in libraries to use
suppressMessages(library(tidyverse))
suppressMessages(library(DataExplorer))
suppressMessages(library(modelr))
suppressMessages(library(lubridate))
suppressMessages(library(stringr))
suppressMessages(library(purrr))
suppressMessages(library(gganimate))
suppressMessages(library(readxl))
suppressMessages(library(gifski))
suppressMessages(library(tidycensus))
suppressMessages(library(rvest))
suppressMessages(library(tmap))
suppressMessages(library(spData))
suppressMessages(library(tigris))
suppressMessages(library(sf))
suppressMessages(library(gridExtra))
suppressMessages(library(corrplot))
# 11/8: Added packages "tmap", "spData", "tigris"
#install.packages("tmap")
#install.packages("spData")
#install.packages("tigris")
# set up a data directory if it does not exist already
if(!file.exists("./data")) {dir.create("./data")}
# store urls needed - right click on excel symbol and select
# copy link address
eviction_US_all <- c("https://eviction-lab-data-downloads.s3.amazonaws.com/US/all.csv")
# check to see if URL saved to variable
eviction_US_all
#download the csv file
download.file(eviction_US_all, destfile = "./data/eviction_US_all.csv", mode = "wb") #this takes a bit ranges from 2-5 minutes, over bad WIFI might take 10 min
# read the csv file into R and save into dataframe
eviction_US_all <- read_csv("./data/eviction_US_all.csv")
eviction_US_all
# download and load urbal/rural classifications
# set up a data directory if it does not exist already
if(!file.exists("./data")) {dir.create("./data")}
# store urls needed - right click on excel symbol and select
# copy link address
urban_rural <- c("http://www2.census.gov/geo/docs/reference/ua/County_Rural_Lookup.xlsx")
#download the .xlsx file
download.file(urban_rural, destfile = "./data/urban_rural.xlsx", mode = "wb")
#read xlsx into R
urban_rural <- read_excel(skip = 3, "./data/urban_rural.xlsx")
names(urban_rural)
# clean data set
urban_rural <- urban_rural %>%
rename("Percent_Rural" = "2010 Census \r\nPercent Rural",
"GEOID" = "2015 GEOID")
# check names were updated
names(urban_rural)
# select only needed fields
urban_rural <-urban_rural %>%
mutate(rural_flag = ifelse(round(Percent_Rural, 3) > 50, 1, 0)) %>%
select(GEOID, State, Percent_Rural, rural_flag)
# view cleaned up dataset
urban_rural
if(!file.exists("./data/labor_data")) {dir.create("./data/labor_data")}
#save url into a variable
url <- "https://www.bls.gov/lau/#tables"
#download the html content using read_html
download.file(url,destfile="./data/uslabor.html")
us_county_labor_html <- read_html("./data/uslabor.html")
#extract the xslx
us_county_labor_html %>%
rvest::html_nodes("ul") %>%
rvest::html_nodes("li") %>%
rvest::html_nodes("a") %>%
rvest::html_attr("href") %>%
str_subset(".xlsx$") -> us_labor_urls
#domain
domain <- "https://www.bls.gov"
#paste domain to urls
str_c(domain,us_labor_urls) -> us_labor_urls
#only need years from 2000 to 2016
us_labor_urls[3:19] -> us_labor_2000_2016
years <- rep(2000:2016,1)
#a for loop that downloads each file
for(i in seq_along(us_labor_2000_2016)){
download.file(us_labor_2000_2016[i],destfile = paste("./data/labor_data/",years[i],".xslx",sep=""),mode="wb")
}
#save the files pertaining to us labor
labor_files <- dir("./data/labor_data")
#create a function that downloads each url and saves it #into a dataframe
read_files <- function(x){
read_excel(path= paste("./data/labor_data/",x,sep=""),skip = 7,col_names = c("laus_code","state_fips_code","county_fips_code","county_name","year","","labor_force","employed","unemployed","unemployment_rate"),sheet = 1,na="")
}
#map the function to read each file
map(labor_files,read_files) -> all_labor_data
#join all the US labor tables
all_labor_data %>% reduce(full_join) -> all_labor_data
#remove some rows that have NA in Year and remove empty column next to year
filter(all_labor_data,!is.na(year)) %>% select(-X__1) -> all_labor_data
#remove some rows that have NA in Year and remove empty column next to year
filter(all_labor_data,!is.na(year)) %>% select(-...6) -> all_labor_data
#creating a GEOID to join
all_labor_data$GEOID <- str_c(all_labor_data$state_fips_code,all_labor_data$county_fips_code)
#change year to integer
all_labor_data$year <- as.integer(all_labor_data$year)
# set up a data directory if it does not exist already
if(!file.exists("./data")) {dir.create("./data")}
#download county shapefiles
download.file("https://www2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_county_500k.zip", destfile = "./data/county_shp.zip")
spatial_data <- "./data/county_shp.zip"
unzip(spatial_data, overwrite = TRUE, exdir = "./data/spatial/county_shp")
county_boundaries <- st_read("./data/spatial/county_shp/cb_2018_us_county_500k.shp")
county_boundaries
#County Table joined with 2 additional variables : rural flag and unemployment rate
eviction_county <- eviction_US_all %>%
right_join(urban_rural, key = "GEOID") %>%
left_join(select(all_labor_data,GEOID,year,unemployment_rate), by =c("GEOID" = "GEOID", "year"="year"))
eviction_county
eviction_state <- eviction_US_all %>%
filter(nchar(GEOID) == 2)
eviction_state
#look at col names
colnames(eviction_US_all)
#look at unique values for each variable
map(eviction_US_all, unique)
#We have 22.5% missing of eviction rate data in county and 11% missing in state. ALl
missing_state <- plot_missing(eviction_state)
missing_eviction <- plot_missing(eviction_county)
#colSums(is.na(eviction_state))/nrow(eviction_state)
#colSums(is.na(eviction_county))/nrow(eviction_county)
grid.arrange(missing_state,missing_eviction,ncol=2)
#plot_correlation(na.omit(eviction_county), maxcat = 5L)
na.omit(eviction_county) %>%
select_if(is.numeric) %>%
as.matrix() %>% cor()  -> cor_matrix
corrplot(cor_matrix[,1:nrow(cor_matrix)][1:nrow(cor_matrix),20, drop=FALSE], cl.pos='n',cl.ratio = .2, method = "number", tl.srt = 45,tl.col = "black")
#corrplot(cor_matrix, type="upper", order="hclust", sig.level = 0.01, insig = "blank")
#creating a new variable of nonwhite percentage
eviction_county$pct_nonwhite <- 100 - eviction_county$`pct-white`
na.omit(eviction_county) %>%
select_if(is.numeric) %>%
as.matrix() %>% cor()  -> cor_matrix
#look at col names
colnames(eviction_US_all)
#look at summary of data
summary(eviction_US_all)
summary(all_labor_data)
