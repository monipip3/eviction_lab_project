geom_polygon(colour='black',fill='white') +
facet_wrap(~region
, scales = "free"
, ncol=3)
ggplot(map_data('state', region=states)
, aes(x=long, y = lay, group = group)) +
geom_polygon(fill = "white", colour = "black") +
facet_wrap(~region, scales = "free", nol = 3)
ggplot(map_data('state', region = states),
aes(x = long, y = lay, group = group)) +
geom_polygon(fill = "white", colour = "black") +
facet_wrap(~region, scales = "free", nol = 3)
ggplot(map_data('state', region = states),
aes(x = long, y = lat, group = group)) +
geom_polygon(fill = "white", colour = "black") +
facet_wrap(~region, scales = "free", nol = 3)
filing_by_county
filing_by_state
eviction_avg
eviction_avg_state<- merge(eviction_state, eviction_avg, by="GEOID")
eviction_avg_state
eviction_avg_state
eviction_avg_state
eviction_avg
eviction_avg_state
# get Census TIGER shapefiles for state
state_us_geo <- tigris::states(class= "sf")
# add spatial data to the state level eviction data
eviction_by_state <- state_us_geo %>%
left_join(eviction_state, c("GEOID" = "GEOID"))
# remove states that are not in the continental US
eviction_avg <- eviction_by_state %>%
group_by(GEOID) %>%
summarise(avg_evic_rate = mean(eviction_rate, na.rm = TRUE)) %>%
filter(!GEOID %in% c('02', '15', '60', '66', '69', '72', '78'))
filing_by_state <- eviction_by_state %>%
group_by(GEOID) %>%
summarise(avg_filing_rate = mean(eviction_filing_rate, na.rm = TRUE)) %>%
filter(!GEOID %in% c('02', '15', '60', '66', '69', '72', '78'))
# merge new field into table
filing_avg_state<- merge(filing_by_state, eviction_state,  by="GEOID")
filing_avg_state
# confirm the unwanted GEOIDs were removed
filing_by_state %>%
distinct(GEOID)
# add spatial data to the county level eviction data
eviction_by_county <- county_boundaries %>%
left_join(eviction_county, c("GEOID" = "GEOID"))
# ensure spatial data was added
head(eviction_by_county)
filing_by_county <- eviction_by_county %>%
filter(!STATEFP %in% c('02', '15', '60', '66', '69', '72', '78')) %>%
group_by(GEOID) %>%
summarise(avg_filing_rate = mean(eviction_filing_rate, na.rm = TRUE))
filing_avg_county <- merge(filing_by_county, eviction_county, by = "GEOID")
filing_avg_state
filing_avg_county
filing_avg_state
filing_avg_county
filing_avg_county$unemployment_rate
summarized_filing_county <- filing_avg_county %>%
summarise_all()
summarized_filing_county <- filing_avg_county %>%
summarise_all(mean())
summarized_filing_county <- filing_avg_county %>%
summarise_if(is.numeric, mean, na.rm = TRUE)
summarized_filing_county
summarized_filing_county <- filing_avg_county %>%
filter(!STATEFP %in% c('02', '15', '60', '66', '69', '72', '78'))
save.image(file='myEnvironment.Documents/GitHub')
# call in libraries to use
suppressMessages(library(tidyverse))
suppressMessages(library(DataExplorer))
suppressMessages(library(modelr))
suppressMessages(library(lubridate))
suppressMessages(library(stringr))
suppressMessages(library(purrr))
suppressMessages(library(gganimate))
suppressMessages(library(readxl))
suppressMessages(library(gifski))
suppressMessages(library(tidycensus))
suppressMessages(library(rvest))
suppressMessages(library(tmap))
suppressMessages(library(spData))
suppressMessages(library(tigris))
suppressMessages(library(sf))
suppressMessages(library(gridExtra))
suppressMessages(library(corrplot))
suppressMessages(library(maps))
# 11/8: Added packages "tmap", "spData", "tigris"
#install.packages("tmap")
#install.packages("spData")
#install.packages("tigris")
#install.packages("DataExplorer")
#install.packages("gridExtra")
#install.packages("corrplot")
#11/11: Added packages
#install.packages("maps")
# set up a data directory if it does not exist already
if(!file.exists("./data")) {dir.create("./data")}
# store urls needed - right click on excel symbol and select
# copy link address
eviction_US_all <- c("https://eviction-lab-data-downloads.s3.amazonaws.com/US/all.csv")
# check to see if URL saved to variable
eviction_US_all
#download the csv file
download.file(eviction_US_all, destfile = "./data/eviction_US_all.csv", mode = "wb") #this takes a bit ranges from 2-5 minutes, over bad WIFI might take 10 min
# read the csv file into R and save into dataframe
eviction_US_all <- read_csv("./data/eviction_US_all.csv")
# clean up variable names
names(eviction_US_all) <- gsub(x = names(eviction_US_all), pattern = "-", replacement = "_")
# view datasets
eviction_US_all
# download and load urbal/rural classifications
# set up a data directory if it does not exist already
if(!file.exists("./data")) {dir.create("./data")}
# store urls needed - right click on excel symbol and select
# copy link address
urban_rural <- c("http://www2.census.gov/geo/docs/reference/ua/County_Rural_Lookup.xlsx")
#download the .xlsx file
download.file(urban_rural, destfile = "./data/urban_rural.xlsx", mode = "wb")
#read xlsx into R
urban_rural <- read_excel(skip = 3, "./data/urban_rural.xlsx")
names(urban_rural)
# clean data set
urban_rural <- urban_rural %>%
rename("Percent_Rural" = "2010 Census \r\nPercent Rural",
"GEOID" = "2015 GEOID")
# check names were updated
names(urban_rural)
# select only needed fields
urban_rural <-urban_rural %>%
mutate(rural_flag = ifelse(round(Percent_Rural, 3) > 50, 1, 0)) %>%
select(GEOID, State, Percent_Rural, rural_flag)
# view cleaned up dataset
urban_rural
if(!file.exists("./data/labor_data")) {dir.create("./data/labor_data")}
#save url into a variable
url <- "https://www.bls.gov/lau/#tables"
#download the html content using read_html
download.file(url,destfile="./data/uslabor.html")
us_county_labor_html <- read_html("./data/uslabor.html")
#extract the xslx
us_county_labor_html %>%
rvest::html_nodes("ul") %>%
rvest::html_nodes("li") %>%
rvest::html_nodes("a") %>%
rvest::html_attr("href") %>%
str_subset(".xlsx$") -> us_labor_urls
#domain
domain <- "https://www.bls.gov"
#paste domain to urls
str_c(domain,us_labor_urls) -> us_labor_urls
#only need years from 2000 to 2016
us_labor_urls[3:19] -> us_labor_2000_2016
years <- rep(2000:2016,1)
#a for loop that downloads each file
for(i in seq_along(us_labor_2000_2016)){
download.file(us_labor_2000_2016[i],destfile = paste("./data/labor_data/",years[i],".xslx",sep=""),mode="wb")
}
#save the files pertaining to us labor
labor_files <- dir("./data/labor_data")
#create a function that downloads each url and saves it #into a dataframe
read_files <- function(x){
read_excel(path= paste("./data/labor_data/",x,sep=""),skip = 7,col_names = c("laus_code","state_fips_code","county_fips_code","county_name","year","","labor_force","employed","unemployed","unemployment_rate"),sheet = 1,na="")
}
#map the function to read each file
map(labor_files,read_files) -> all_labor_data
read_files <- function(x){
read_excel(path= paste("./data/labor_data/",x,sep=""),skip = 7,col_names = c("laus_code","state_fips_code","county_fips_code","county_name","year","","labor_force","employed","unemployed","unemployment_rate"),sheet = 1,na="")
}
#map the function to read each file
map(labor_files,read_files) -> all_labor_data
all_labor_data %>% reduce(full_join) -> all_labor_data
if(!file.exists("./data/labor_data")) {dir.create("./data/labor_data")}
#save url into a variable
url <- "https://www.bls.gov/lau/#tables"
#download the html content using read_html
download.file(url,destfile="./data/uslabor.html")
us_county_labor_html <- read_html("./data/uslabor.html")
#extract the xslx
us_county_labor_html %>%
rvest::html_nodes("ul") %>%
rvest::html_nodes("li") %>%
rvest::html_nodes("a") %>%
rvest::html_attr("href") %>%
str_subset(".xlsx$") -> us_labor_urls
#domain
domain <- "https://www.bls.gov"
#paste domain to urls
str_c(domain,us_labor_urls) -> us_labor_urls
#only need years from 2000 to 2016
us_labor_urls[3:19] -> us_labor_2000_2016
years <- rep(2000:2016,1)
#a for loop that downloads each file
for(i in seq_along(us_labor_2000_2016)){
download.file(us_labor_2000_2016[i],destfile = paste("./data/labor_data/",years[i],".xslx",sep=""),mode="wb")
}
#save the files pertaining to us labor
labor_files <- dir("./data/labor_data")
#create a function that downloads each url and saves it #into a dataframe
read_files <- function(x){
read_excel(path= paste("./data/labor_data/",x,sep=""),skip = 7,col_names = c("laus_code","state_fips_code","county_fips_code","county_name","year","","labor_force","employed","unemployed","unemployment_rate"),sheet = 1,na="")
}
#map the function to read each file
map(labor_files,read_files) -> all_labor_data
#remove some rows that have NA in Year and remove empty column next to year
filter(all_labor_data,!is.na(year)) %>% select(-6) -> all_labor_data
map(labor_files,read_files) -> all_labor_data
labor_files <- dir("./data/labor_data")
read_files <- function(x){
read_excel(path= paste("./data/labor_data/",x,sep=""),skip = 7,col_names = c("laus_code","state_fips_code","county_fips_code","county_name","year","","labor_force","employed","unemployed","unemployment_rate"),sheet = 1,na="")
}
map(labor_files,read_files) -> all_labor_data
read_files <- function(x){
read_excel(path= paste("./data/labor_data/",x,sep=""),skip = 7,col_names = c("laus_code","state_fips_code","county_fips_code","county_name","year","","labor_force","employed","unemployed","unemployment_rate"),sheet = 1,na="")
}
labor_files <- dir("./data/labor_data")
map(labor_files,read_files) -> all_labor_data
if(!file.exists("./data/labor_data")) {dir.create("./data/labor_data")}
#save url into a variable
url <- "https://www.bls.gov/lau/#tables"
#download the html content using read_html
download.file(url,destfile="./data/uslabor.html")
us_county_labor_html <- read_html("./data/uslabor.html")
#extract the xslx
us_county_labor_html %>%
rvest::html_nodes("ul") %>%
rvest::html_nodes("li") %>%
rvest::html_nodes("a") %>%
rvest::html_attr("href") %>%
str_subset(".xlsx$") -> us_labor_urls
#domain
domain <- "https://www.bls.gov"
#paste domain to urls
str_c(domain,us_labor_urls) -> us_labor_urls
#only need years from 2000 to 2016
us_labor_urls[3:19] -> us_labor_2000_2016
years <- rep(2000:2016,1)
#a for loop that downloads each file
for(i in seq_along(us_labor_2000_2016)){
download.file(us_labor_2000_2016[i],destfile = paste("./data/labor_data/",years[i],".xslx",sep=""),mode="wb")
}
#save the files pertaining to us labor
labor_files <- dir("./data/labor_data")
#create a function that downloads each url and saves it #into a dataframe
read_files <- function(x){
read_excel(path= paste("./data/labor_data/",x,sep=""),skip = 7,col_names = c("laus_code","state_fips_code","county_fips_code","county_name","year","","labor_force","employed","unemployed","unemployment_rate"),sheet = 1,na="")
}
#map the function to read each file
map(labor_files,read_files) -> all_labor_data
labor_files
getwd()
getwd()
getwd()
getwd()
getwd()
```{r Webscrape US Labor Stats}
getwd()
if(!file.exists("./data/labor_data")) {dir.create("./data/labor_data")}
#save url into a variable
url <- "https://www.bls.gov/lau/#tables"
#download the html content using read_html
download.file(url,destfile="./data/uslabor.html")
us_county_labor_html <- read_html("./data/uslabor.html")
#extract the xslx
us_county_labor_html %>%
rvest::html_nodes("ul") %>%
rvest::html_nodes("li") %>%
rvest::html_nodes("a") %>%
rvest::html_attr("href") %>%
str_subset(".xlsx$") -> us_labor_urls
#domain
domain <- "https://www.bls.gov"
#paste domain to urls
str_c(domain,us_labor_urls) -> us_labor_urls
#only need years from 2000 to 2016
us_labor_urls[3:19] -> us_labor_2000_2016
years <- rep(2000:2016,1)
#a for loop that downloads each file
for(i in seq_along(us_labor_2000_2016)){
download.file(us_labor_2000_2016[i],destfile = paste("./data/labor_data/",years[i],".xslx",sep=""),mode="wb")
}
#save the files pertaining to us labor
labor_files <- dir("./data/labor_data")
read_files <- function(x){
read_excel(path= paste("./data/labor_data/",x,sep=""),skip = 7,col_names = c("laus_code","state_fips_code","county_fips_code","county_name","year","","labor_force","employed","unemployed","unemployment_rate"),sheet = 1,na="")
}
#map the function to read each file
map(labor_files,read_files) -> all_labor_data
getwd()
getwd()
#map the function to read each file
map(labor_files,read_files) -> all_labor_data
all_labor_data <- map(labor_files, read_files)
getwd()
if(!file.exists("./data/labor_data")) {dir.create("./data/labor_data")}
#save url into a variable
url <- "https://www.bls.gov/lau/#tables"
#download the html content using read_html
download.file(url,destfile="./data/uslabor.html")
us_county_labor_html <- read_html("./data/uslabor.html")
#extract the xslx
us_county_labor_html %>%
rvest::html_nodes("ul") %>%
rvest::html_nodes("li") %>%
rvest::html_nodes("a") %>%
rvest::html_attr("href") %>%
str_subset(".xlsx$") -> us_labor_urls
#domain
domain <- "https://www.bls.gov"
#paste domain to urls
str_c(domain,us_labor_urls) -> us_labor_urls
#only need years from 2000 to 2016
us_labor_urls[3:19] -> us_labor_2000_2016
years <- rep(2000:2016,1)
#a for loop that downloads each file
for(i in seq_along(us_labor_2000_2016)){
download.file(us_labor_2000_2016[i],destfile = paste("./data/labor_data/",years[i],".xslx",sep=""),mode="wb")
}
#save the files pertaining to us labor
labor_files <- dir("./data/labor_data")
getwd()
#create a function that downloads each url and saves it #into a dataframe
read_files <- function(x){
read_excel(path= paste("./data/labor_data/",x,sep=""),skip = 7,col_names = c("laus_code","state_fips_code","county_fips_code","county_name","year","","labor_force","employed","unemployed","unemployment_rate"),sheet = 1,na="")
}
#map the function to read each file
all_labor_data <- map(labor_files, read_files)
#only need years from 2000 to 2016
us_labor_urls[3:19] -> us_labor_2000_2016
years <- rep(2000:2016,1)
years <- as.character(years)
#a for loop that downloads each file
for(i in seq_along(us_labor_2000_2016)){
download.file(us_labor_2000_2016[i],destfile = paste("./data/labor_data/y",years[i],".xslx",sep=""),mode="wb")
}
#save the files pertaining to us labor
labor_files <- dir("./data/labor_data")
getwd()
#create a function that downloads each url and saves it #into a dataframe
read_files <- function(x){
read_excel(path= paste("./data/labor_data/",x,sep=""),skip = 7,col_names = c("laus_code","state_fips_code","county_fips_code","county_name","year","","labor_force","employed","unemployed","unemployment_rate"),sheet = 1, na="")
}
#map the function to read each file
all_labor_data <- map(labor_files, read_files)
read_files <- function(x){
read_excel(path= paste("./data/labor_data/y",x,sep=""),skip = 7,col_names = c("laus_code","state_fips_code","county_fips_code","county_name","year","","labor_force","employed","unemployed","unemployment_rate"),sheet = 1, na="")
}
#map the function to read each file
all_labor_data <- map(labor_files, read_files)
labor_files <- dir("./data/labor_data")
read_files <- function(x){
read_excel(path= paste("./data/labor_data/y",x,sep=""),skip = 7,col_names = c("laus_code","state_fips_code","county_fips_code","county_name","year","","labor_force","employed","unemployed","unemployment_rate"),sheet = 1, na="")
}
#map the function to read each file
all_labor_data <- map(labor_files, read_files)
suppressMessages(library(tidyverse))
suppressMessages(library(DataExplorer))
suppressMessages(library(modelr))
suppressMessages(library(lubridate))
suppressMessages(library(stringr))
suppressMessages(library(gganimate))
suppressMessages(library(readxl))
suppressMessages(library(gifski))
suppressMessages(library(tidycensus))
suppressMessages(library(rvest))
suppressMessages(library(tmap))
suppressMessages(library(spData))
suppressMessages(library(tigris))
suppressMessages(library(sf))
suppressMessages(library(gridExtra))
suppressMessages(library(corrplot))
suppressMessages(library(maps))
suppressMessages(library(purrr))
map(labor_files, read_files) -> all_labor_data
uninstall.packages("maps")
remove.packages(maps)
suppressMessages(library(tidyverse))
suppressMessages(library(DataExplorer))
suppressMessages(library(modelr))
suppressMessages(library(lubridate))
suppressMessages(library(stringr))
suppressMessages(library(gganimate))
suppressMessages(library(readxl))
suppressMessages(library(gifski))
suppressMessages(library(tidycensus))
suppressMessages(library(rvest))
suppressMessages(library(tmap))
suppressMessages(library(spData))
suppressMessages(library(tigris))
suppressMessages(library(sf))
suppressMessages(library(gridExtra))
suppressMessages(library(corrplot))
suppressMessages(library(purrr))
getwd()
if(!file.exists("./data/labor_data")) {dir.create("./data/labor_data")}
#save url into a variable
url <- "https://www.bls.gov/lau/#tables"
#download the html content using read_html
download.file(url,destfile="./data/uslabor.html")
us_county_labor_html <- read_html("./data/uslabor.html")
#extract the xslx
us_county_labor_html %>%
rvest::html_nodes("ul") %>%
rvest::html_nodes("li") %>%
rvest::html_nodes("a") %>%
rvest::html_attr("href") %>%
str_subset(".xlsx$") -> us_labor_urls
#domain
domain <- "https://www.bls.gov"
#paste domain to urls
str_c(domain,us_labor_urls) -> us_labor_urls
#only need years from 2000 to 2016
us_labor_urls[3:19] -> us_labor_2000_2016
years <- rep(2000:2016,1)
years <- as.character(years)
#a for loop that downloads each file
for(i in seq_along(us_labor_2000_2016)){
download.file(us_labor_2000_2016[i],destfile = paste("./data/labor_data/y",years[i],".xslx",sep=""),mode="wb")
}
#save the files pertaining to us labor
labor_files <- dir("./data/labor_data")
getwd()
#create a function that downloads each url and saves it #into a dataframe
read_files <- function(x){
read_excel(path= paste("./data/labor_data/y",x,sep=""),skip = 7,col_names = c("laus_code","state_fips_code","county_fips_code","county_name","year","","labor_force","employed","unemployed","unemployment_rate"),sheet = 1, na="")
}
#map the function to read each file
map(labor_files, read_files) -> all_labor_data
if(!file.exists("./data/labor_data")) {dir.create("./data/labor_data")}
#save url into a variable
url <- "https://www.bls.gov/lau/#tables"
#download the html content using read_html
download.file(url,destfile="./data/uslabor.html")
us_county_labor_html <- read_html("./data/uslabor.html")
#extract the xslx
us_county_labor_html %>%
rvest::html_nodes("ul") %>%
rvest::html_nodes("li") %>%
rvest::html_nodes("a") %>%
rvest::html_attr("href") %>%
str_subset(".xlsx$") -> us_labor_urls
#domain
domain <- "https://www.bls.gov"
#paste domain to urls
str_c(domain,us_labor_urls) -> us_labor_urls
#only need years from 2000 to 2016
us_labor_urls[3:19] -> us_labor_2000_2016
years <- rep(2000:2016,1)
years <- as.character(years)
#a for loop that downloads each file
for(i in seq_along(us_labor_2000_2016)){
download.file(us_labor_2000_2016[i],destfile = paste("./data/labor_data/y",years[i],".xslx",sep=""),mode="wb")
}
#save the files pertaining to us labor
labor_files <- dir("./data/labor_data")
getwd()
#create a function that downloads each url and saves it #into a dataframe
read_files <- function(x){
read_excel(path= paste("./data/labor_data/y",x,sep=""),skip = 7,col_names = c("laus_code","state_fips_code","county_fips_code","county_name","year","","labor_force","employed","unemployed","unemployment_rate"),sheet = 1, na="")
}
#map the function to read each file
map(labor_files, read_files) -> all_labor_data
getwd()
# call in libraries to use
suppressMessages(library(tidyverse))
suppressMessages(library(DataExplorer))
suppressMessages(library(modelr))
suppressMessages(library(lubridate))
suppressMessages(library(stringr))
suppressMessages(library(gganimate))
suppressMessages(library(readxl))
suppressMessages(library(gifski))
suppressMessages(library(tidycensus))
suppressMessages(library(rvest))
suppressMessages(library(tmap))
suppressMessages(library(spData))
suppressMessages(library(tigris))
suppressMessages(library(sf))
suppressMessages(library(gridExtra))
suppressMessages(library(corrplot))
suppressMessages(library(purrr))
# 11/8: Added packages "tmap", "spData", "tigris"
#install.packages("tmap")
#install.packages("spData")
#install.packages("tigris")
#install.packages("DataExplorer")
#install.packages("gridExtra")
#install.packages("corrplot")
#11/11: Added packages
#install.packages("maps")
# set up a data directory if it does not exist already
if(!file.exists("./data")) {dir.create("./data")}
# store urls needed - right click on excel symbol and select
# copy link address
eviction_US_all <- c("https://eviction-lab-data-downloads.s3.amazonaws.com/US/all.csv")
# check to see if URL saved to variable
eviction_US_all
#download the csv file
download.file(eviction_US_all, destfile = "./data/eviction_US_all.csv", mode = "wb") #this takes a bit ranges from 2-5 minutes, over bad WIFI might take 10 min
# read the csv file into R and save into dataframe
eviction_US_all <- read_csv("./data/eviction_US_all.csv")
# set up a data directory if it does not exist already
if(!file.exists("./data")) {dir.create("./data")}
# store urls needed - right click on excel symbol and select
# copy link address
eviction_US_all <- c("https://eviction-lab-data-downloads.s3.amazonaws.com/US/all.csv")
# check to see if URL saved to variable
eviction_US_all
#download the csv file
download.file(eviction_US_all, destfile = "./data/eviction_US_all.csv", mode = "wb") #this takes a bit ranges from 2-5 minutes, over bad WIFI might take 10 min
